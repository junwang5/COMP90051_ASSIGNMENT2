{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain 1: 1000 samples, Class distribution: [500 500]\n",
      "Domain 2: 5000 samples, Class distribution: [ 250 4750]\n"
     ]
    }
   ],
   "source": [
    "def load_json_data(path):\n",
    "    texts, labels = [], []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            sample = json.loads(line)\n",
    "            token_str = ' '.join(map(str, sample['text']))\n",
    "            texts.append(token_str)\n",
    "            labels.append(sample['label'])\n",
    "    return texts, labels\n",
    "\n",
    "def load_test_data(path):\n",
    "    test_texts, test_ids = [], []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            sample = json.loads(line)\n",
    "            text = ' '.join(map(str, sample['text']))\n",
    "            test_texts.append(text)\n",
    "            test_ids.append(sample['id'])\n",
    "    return test_texts, test_ids\n",
    "\n",
    "# Load datasets \n",
    "X1, y1 = load_json_data('../domain1_train_data.json')\n",
    "X2, y2 = load_json_data('../domain2_train_data.json')\n",
    "\n",
    "print(f\"Domain 1: {len(X1)} samples, Class distribution: {np.bincount(y1)}\")\n",
    "print(f\"Domain 2: {len(X2)} samples, Class distribution: {np.bincount(y2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augmentation strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_text_augmentation(texts, labels, augment_ratio=0.3):\n",
    "    \"\"\"Simple text data augmentation: random token shuffling\"\"\"\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for text, label in zip(texts, labels):\n",
    "        # Original data\n",
    "        augmented_texts.append(text)\n",
    "        augmented_labels.append(label)\n",
    "        \n",
    "        # Data augmentation with random probability\n",
    "        if random.random() < augment_ratio:\n",
    "            tokens = text.split()\n",
    "            if len(tokens) > 3:  # Only augment sufficiently long texts\n",
    "                # Strategy: randomly shuffle partial tokens (preserve most structure)\n",
    "                shuffle_ratio = 0.2  # Only shuffle 20% of tokens\n",
    "                n_shuffle = max(1, int(len(tokens) * shuffle_ratio))\n",
    "                indices = random.sample(range(len(tokens)), min(n_shuffle, len(tokens)))\n",
    "                \n",
    "                tokens_copy = tokens.copy()\n",
    "                shuffled_tokens = [tokens_copy[i] for i in indices]\n",
    "                random.shuffle(shuffled_tokens)\n",
    "                \n",
    "                for i, idx in enumerate(indices):\n",
    "                    tokens_copy[idx] = shuffled_tokens[i]\n",
    "                \n",
    "                augmented_text = ' '.join(tokens_copy)\n",
    "                augmented_texts.append(augmented_text)\n",
    "                augmented_labels.append(label)\n",
    "    \n",
    "    return augmented_texts, augmented_labels\n",
    "\n",
    "# Split domains into train/validation\n",
    "X1_train, X1_val, y1_train, y1_val = train_test_split(\n",
    "    X1, y1, test_size=0.2, random_state=42, stratify=y1\n",
    ")\n",
    "X2_train, X2_val, y2_train, y2_val = train_test_split(\n",
    "    X2, y2, test_size=0.2, random_state=42, stratify=y2\n",
    ")\n",
    "\n",
    "# Apply data augmentation\n",
    "X1_train_aug, y1_train_aug = simple_text_augmentation(X1_train, y1_train, augment_ratio=0.4)\n",
    "X2_train_aug, y2_train_aug = simple_text_augmentation(X2_train, y2_train, augment_ratio=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conservative feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dimensions: 3000\n"
     ]
    }
   ],
   "source": [
    "X_train_combined = X1_train_aug + X2_train_aug\n",
    "\n",
    "# TF-IDF vectorization with conservative parameters\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=3000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_df=0.95,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "tfidf_vectorizer.fit(X_train_combined)\n",
    "\n",
    "# Transform all data\n",
    "X1_train_vec = tfidf_vectorizer.transform(X1_train_aug).toarray()\n",
    "X2_train_vec = tfidf_vectorizer.transform(X2_train_aug).toarray()\n",
    "X1_val_vec = tfidf_vectorizer.transform(X1_val).toarray()\n",
    "X2_val_vec = tfidf_vectorizer.transform(X2_val).toarray()\n",
    "\n",
    "print(f\"Feature dimensions: {X1_train_vec.shape[1]}\")\n",
    "\n",
    "# Merge training data\n",
    "X_train_all = np.vstack([X1_train_vec, X2_train_vec])\n",
    "y_train_all = np.hstack([y1_train_aug, y2_train_aug])\n",
    "\n",
    "# Feature selection\n",
    "selector = SelectKBest(chi2, k=min(2000, X_train_all.shape[1]))\n",
    "X_train_selected = selector.fit_transform(X_train_all, y_train_all)\n",
    "X1_val_selected = selector.transform(X1_val_vec)\n",
    "X2_val_selected = selector.transform(X2_val_vec)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X1_val_np = X1_val_selected.astype(np.float32)\n",
    "X2_val_np = X2_val_selected.astype(np.float32)\n",
    "y1_val_np = np.array(y1_val, dtype=np.int64)\n",
    "y2_val_np = np.array(y2_val, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal DANN model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension: 2000\n",
      "Model parameters: 320804\n"
     ]
    }
   ],
   "source": [
    "class GradientReversalLayer(torch.autograd.Function):\n",
    "    \"\"\"Gradient Reversal Layer for adversarial training\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, alpha):\n",
    "        ctx.alpha = alpha\n",
    "        return x\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return -ctx.alpha * grad_output, None\n",
    "\n",
    "class OptimalDANN(nn.Module):\n",
    "    \"\"\"Optimal Domain Adversarial Neural Network with conservative architecture\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim=160, dropout_rate=0.1):\n",
    "        super(OptimalDANN, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "        self.label_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "        \n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, alpha=0.03):\n",
    "        features = self.feature_extractor(x)\n",
    "        label_output = self.label_classifier(features)\n",
    "        \n",
    "        reversed_features = GradientReversalLayer.apply(features, alpha)\n",
    "        domain_output = self.domain_classifier(reversed_features)\n",
    "        \n",
    "        return label_output, domain_output\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train_selected.shape[1]\n",
    "model = OptimalDANN(\n",
    "    input_dim=input_dim,\n",
    "    hidden_dim=160,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X1_val, y1_val, X2_val, y2_val):\n",
    "    \"\"\"Evaluate model on validation sets\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Domain 1 validation\n",
    "        X1_tensor = torch.FloatTensor(X1_val)\n",
    "        label_out1, _ = model(X1_tensor, alpha=0.0)\n",
    "        pred1 = torch.argmax(label_out1, dim=1)\n",
    "        acc1 = accuracy_score(y1_val, pred1.numpy())\n",
    "        \n",
    "        # Domain 2 validation\n",
    "        X2_tensor = torch.FloatTensor(X2_val)\n",
    "        label_out2, _ = model(X2_tensor, alpha=0.0)\n",
    "        pred2 = torch.argmax(label_out2, dim=1)\n",
    "        acc2 = accuracy_score(y2_val, pred2.numpy())\n",
    "        \n",
    "        # Weighted accuracy\n",
    "        weighted_acc = 0.60 * acc1 + 0.4 * acc2\n",
    "        \n",
    "    model.train()\n",
    "    return weighted_acc, acc1, acc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_augmented_dann(model, X_train, y_train, X1_val, y1_val, X2_val, y2_val):\n",
    "    \"\"\"Training function with optimal hyperparameters\"\"\"\n",
    "    \n",
    "    # Prepare domain labels (adjusted for augmented data)\n",
    "    n_domain1_aug = len([x for x in y_train if x in y1_train_aug])\n",
    "    d_train = np.hstack([\n",
    "        np.zeros(n_domain1_aug),\n",
    "        np.ones(len(X_train) - n_domain1_aug)\n",
    "    ])\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(X_train)\n",
    "    y_tensor = torch.LongTensor(y_train)\n",
    "    d_tensor = torch.LongTensor(d_train)\n",
    "    \n",
    "    dataset = TensorDataset(X_tensor, y_tensor, d_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0012, weight_decay=1e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    best_val_acc = 0.0\n",
    "    best_d1_acc = 0.0\n",
    "    best_d2_acc = 0.0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(20):\n",
    "        total_loss = 0\n",
    "        total_label_loss = 0\n",
    "        total_domain_loss = 0\n",
    "        \n",
    "        for batch_X, batch_y, batch_d in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            label_output, domain_output = model(batch_X, alpha=0.03)\n",
    "            \n",
    "            label_loss = criterion(label_output, batch_y)\n",
    "            domain_loss = criterion(domain_output, batch_d)\n",
    "            \n",
    "            total_batch_loss = label_loss + 0.05 * domain_loss\n",
    "            \n",
    "            total_batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += total_batch_loss.item()\n",
    "            total_label_loss += label_loss.item()\n",
    "            total_domain_loss += domain_loss.item()\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            val_acc, d1_acc, d2_acc = evaluate_model(\n",
    "                model, X1_val, y1_val, X2_val, y2_val\n",
    "            )\n",
    "            \n",
    "            print(f\"Epoch [{epoch+1:2d}/20], Loss: {total_loss/len(dataloader):.4f}, \"\n",
    "                  f\"Label Loss: {total_label_loss/len(dataloader):.4f}, \"\n",
    "                  f\"Domain Loss: {total_domain_loss/len(dataloader):.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}\")\n",
    "            \n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                best_d1_acc = d1_acc\n",
    "                best_d2_acc = d2_acc\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), 'best_augmented_dann.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if patience_counter >= 3:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_augmented_dann.pth'))\n",
    "    return model, best_val_acc, best_d1_acc, best_d2_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented DANN training...\n",
      "Epoch [ 5/20], Loss: 0.0254, Label Loss: 0.0252, Domain Loss: 0.0037, Val Acc: 0.9316\n",
      "Epoch [10/20], Loss: 0.0054, Label Loss: 0.0053, Domain Loss: 0.0015, Val Acc: 0.9282\n",
      "Epoch [15/20], Loss: 0.0025, Label Loss: 0.0024, Domain Loss: 0.0011, Val Acc: 0.9342\n",
      "Epoch [20/20], Loss: 0.0015, Label Loss: 0.0014, Domain Loss: 0.0010, Val Acc: 0.9372\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting augmented DANN training...\")\n",
    "\n",
    "trained_model, best_accuracy, best_d1_acc, best_d2_acc = train_augmented_dann(\n",
    "    model,\n",
    "    X_train_selected, y_train_all,\n",
    "    X1_val_np, y1_val_np,\n",
    "    X2_val_np, y2_val_np\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DNN_18 Data Augmentation Results ===\n",
      "Validation accuracy: 0.9372\n",
      "Domain 1 accuracy: 0.9100 (91.0%)\n",
      "Domain 2 accuracy: 0.9780 (97.8%)\n",
      "Model parameters: 320,804\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== DNN_18 Data Augmentation Results ===\")\n",
    "print(f\"Validation accuracy: {best_accuracy:.4f}\")\n",
    "print(f\"Domain 1 accuracy: {best_d1_acc:.4f} ({best_d1_acc:.1%})\")\n",
    "print(f\"Domain 2 accuracy: {best_d2_acc:.4f} ({best_d2_acc:.1%})\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Final validation confirmation\n",
    "final_val_acc, final_d1_acc, final_d2_acc = evaluate_model(\n",
    "    trained_model, X1_val_np, y1_val_np, X2_val_np, y2_val_np\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test predictions and submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape: torch.Size([4000, 2000])\n",
      "Predictions saved to 'dnn_18_data_augmentation_predictions.csv'\n",
      "Prediction distribution: [1884 2116]\n",
      "\n",
      "First 10 predictions:\n",
      "   id  class\n",
      "0   0      1\n",
      "1   1      0\n",
      "2   2      0\n",
      "3   3      1\n",
      "4   4      0\n",
      "5   5      1\n",
      "6   6      0\n",
      "7   7      1\n",
      "8   8      1\n",
      "9   9      1\n"
     ]
    }
   ],
   "source": [
    "# Load and process test data \n",
    "X_test_raw, test_ids = load_test_data('../test_data.json')\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test_raw).toarray()\n",
    "X_test_selected = selector.transform(X_test_tfidf)\n",
    "X_test_tensor = torch.FloatTensor(X_test_selected)\n",
    "\n",
    "print(f\"Test data shape: {X_test_tensor.shape}\")\n",
    "\n",
    "# Generate predictions\n",
    "trained_model.eval()\n",
    "with torch.no_grad():\n",
    "    label_output, _ = trained_model(X_test_tensor, alpha=0.0)\n",
    "    predictions = torch.argmax(label_output, dim=1).tolist()\n",
    "\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'class': predictions\n",
    "})\n",
    "\n",
    "submission_df.to_csv('dnn_18_data_augmentation_predictions.csv', index=False)\n",
    "\n",
    "print(f\"Predictions saved to 'dnn_18_data_augmentation_predictions.csv'\")\n",
    "print(f\"Prediction distribution: {np.bincount(predictions)}\")\n",
    "\n",
    "# Display first few predictions\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(submission_df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
